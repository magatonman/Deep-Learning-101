{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magatonman/Deep-Learning-101/blob/main/hw1_20180389.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS371 Introdution to Deep Learning - Programming Assignment 1\n",
        "\n"
      ],
      "metadata": {
        "id": "SzNL_c6THPKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Guideline"
      ],
      "metadata": {
        "id": "T5xlwpvrHeHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primary TA: Jinwoo Kim (jinwoo-kim@kaist.ac.kr)\n",
        "\n",
        "**How to submit**\n",
        "*   Follow instructions of ðŸ›‘ <mark> TODO</mark> blocks. **DO NOT** modify other parts of the code.\n",
        "*   Submit one file: hw1_{student_ID}.ipynb (*e.g.*, hw1_20231234.ipynb) to KLMS.\n",
        "*   Due date: 23:59:59 Sep 13th (Wed). Late submission accepted until 23:59:59 May 15th (Fri) with 20% penalty.\n",
        "*   **Quiz on assignment 1**: In class, Sep 13th (Wed).\n",
        "\n",
        "**Important notes**\n",
        "*   Do not change the random SEED.\n",
        "*   Use NumPy, do not use ML frameworks like TensorFlow or PyTorch.\n",
        "*   Before submission, check that all cells can be executed sequentially by restarting runtime and running all.\n",
        "*   We will test the correctness of your implementation to score each of the ðŸ›‘ <mark> TODO</mark>.\n",
        "\n",
        "Please feel free to ask questions on the ```#assignment1``` channel on Slack."
      ],
      "metadata": {
        "id": "DjMNJfCEHftM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this programming assignment, you will**\n",
        "* Implement forward computation for Multilayer Perceptron (MLP) neural network.\n",
        "* Implement backpropagation algorithm to compute gradients for the network.\n",
        "* Implement Gradient Descent optimization algorithm to train the network.\n",
        "* Train and test the network on a synthetic dataset for binary classification.\n",
        "\n",
        "**Before submitting, please make sure that you did all the ðŸ›‘<mark>TODOs</mark>**."
      ],
      "metadata": {
        "id": "ja8egfsUHxdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Essentials\n",
        "\n",
        "In this section, we are doing the following:\n",
        "\n",
        "- Installing the required packages\n",
        "- Importing the required packages\n",
        "- Define helper utilities to be used later in the assignment"
      ],
      "metadata": {
        "id": "TsTqsKnGoVu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required packages\n",
        "import numpy as np\n",
        "import random\n",
        "import sklearn.datasets\n",
        "import sklearn.model_selection\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zSHd15NZoa4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "def reset_seed():\n",
        "    np.random.seed(SEED)\n",
        "    random.seed(SEED)\n",
        "\n",
        "\n",
        "def initialize_parameters(input_dim, output_dim):\n",
        "    W = np.random.randn(output_dim, input_dim) * 0.01\n",
        "    b = np.random.randn(output_dim) * 0.01\n",
        "    return W, b\n",
        "\n",
        "\n",
        "def initialize_parameters_xavier(input_dim, output_dim):\n",
        "    W = np.random.randn(output_dim, input_dim) / np.sqrt(input_dim)\n",
        "    b = np.zeros(output_dim)\n",
        "    return W, b\n",
        "\n",
        "# </DO NOT CHANGE>"
      ],
      "metadata": {
        "id": "11s-qYW1okBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Multilayer Perceptron (MLP)\n",
        "\n",
        "## 1.1. Forward Computation\n",
        "\n",
        "In deep learning, our goal is learning some task $\\mathbf{y} = f(\\mathbf{x})$ from input $\\mathbf{x}$ to output $\\mathbf{y}$ by having a parameterized function $\\mathbf{\\hat{y}} = f(\\mathbf{x}; \\theta)$ and learning the parameters $\\theta$ from data.\n",
        "In this assignment, our task is binary classification $\\mathbf{y}\\in\\{0, 1\\}$, and we use a simple Multilayer Perceptron (MLP) neural network as our parameterized function that outputs a prediction $\\mathbf{\\hat{y}}\\in[0, 1]$.\n",
        "\n",
        "The MLP maps inputs $\\mathbf{x}\\in\\mathbb{R}^D$ to output predictions $\\mathbf{\\hat{y}}\\in\\mathbb{R}^C$ where $C=1$ based on two hidden layers as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathbf{h}^{(1)} &= \\mathrm{ReLU}\\left(\\mathbf{W}^{(1)}\\mathbf{x}+\\mathbf{b}^{(1)}\\right),\\\\\n",
        "\\mathbf{h}^{(2)} &= \\mathrm{ReLU}\\left(\\mathbf{W}^{(2)}\\mathbf{h}^{(1)}+\\mathbf{b}^{(2)}\\right),\\\\\n",
        "\\mathbf{\\hat{y}} &= \\mathrm{sigmoid}\\left(\\mathbf{W}^{(3)}\\mathbf{h}^{(2)}+\\mathbf{b}^{(3)}\\right),\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{h}^{(1)},\\mathbf{h}^{(2)}\\in\\mathbb{R}^H$ are activation vectors, $\\mathbf{W}^{(1)}\\in\\mathbb{R}^{H\\times D},\\mathbf{W}^{(2)}\\in\\mathbb{R}^{H\\times H},\\mathbf{W}^{(3)}\\in\\mathbb{R}^{C\\times H}$ are weight matrices, and $\\mathbf{b}^{(1)},\\mathbf{b}^{(2)}\\in\\mathbb{R}^{H},\\mathbf{b}^{(3)}\\in\\mathbb{R}^{C}$ are bias vectors.\n",
        "\n",
        "In addition, $\\mathrm{ReLU}$ and $\\mathrm{sigmoid}$ activation functions are defined as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathrm{ReLU}(\\mathbf{x})_i &= \\max(\\mathbf{x}_i, 0),\\\\\n",
        "\\mathrm{sigmoid}(\\mathbf{x})_i &= \\frac{1}{1 + \\exp{(-\\mathbf{x}_i)}}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "As a whole, the MLP computes a parameterized function $\\mathbf{\\hat{y}} = f(\\mathbf{x}; \\theta)$ with parameters $\\theta = (\\mathbf{W}^{(1)}, \\mathbf{b}^{(1)}, \\mathbf{W}^{(2)}, \\mathbf{b}^{(2)}, \\mathbf{W}^{(3)}, \\mathbf{b}^{(3)})$.\n",
        "\n",
        "ðŸ›‘ <mark> TODO 1</mark>: Implement the forward computation of the MLP."
      ],
      "metadata": {
        "id": "qQeC9CPtoqkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass_linear(x: np.ndarray, W: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Performs the linear transformation.\n",
        "    x: input data (shape: D)\n",
        "    W: weight matrix (shape: H x D)\n",
        "    b: bias vector (shape: H)\n",
        "    Returns:\n",
        "    out: result of linear transformation (shape: H).\n",
        "    \"\"\"\n",
        "    # <TODO>\n",
        "    # Write your implementation here.\n",
        "    # </TODO>\n",
        "    return out\n",
        "\n",
        "\n",
        "def forward_pass_relu(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Performs the ReLU activation.\n",
        "    x: input data (shape: H)\n",
        "    Returns:\n",
        "    out: result of ReLU activation (shape: H).\n",
        "    \"\"\"\n",
        "    # <TODO>\n",
        "    # Write your implementation here.\n",
        "    # </TODO>\n",
        "    return out\n",
        "\n",
        "\n",
        "def forward_pass_sigmoid(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Performs the sigmoid activation.\n",
        "    x: input data (shape: H)\n",
        "    Returns:\n",
        "    out: result of sigmoid activation (shape: H).\n",
        "    \"\"\"\n",
        "    # <TODO>\n",
        "    # Write your implementation here.\n",
        "    # </TODO>\n",
        "    return out"
      ],
      "metadata": {
        "id": "2alUnC3b20hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "\n",
        "def forward_pass(x, W1, b1, W2, b2, W3, b3):\n",
        "    \"\"\"Complete forward pass through the 2-hidden layer MLP.\n",
        "    x: input data (shape: D)\n",
        "    W1, b1: weights and biases for first hidden layer (shape: H x D, H)\n",
        "    W2, b2: weights and biases for second hidden layer (shape: H x H, H)\n",
        "    W3, b3: weights and biases for output layer (shape: H x C, C)\n",
        "    Returns:\n",
        "    out: result of the forward pass (shape: C).\n",
        "    cache: a dictionary containing intermediate outputs of forward pass\n",
        "    \"\"\"\n",
        "    # First hidden layer\n",
        "    z1 = forward_pass_linear(x, W1, b1)\n",
        "    h1 = forward_pass_relu(z1)\n",
        "\n",
        "    # Second hidden layer\n",
        "    z2 = forward_pass_linear(h1, W2, b2)\n",
        "    h2 = forward_pass_relu(z2)\n",
        "\n",
        "    # Output layer\n",
        "    z3 = forward_pass_linear(h2, W3, b3)\n",
        "    out = forward_pass_sigmoid(z3)\n",
        "\n",
        "    cache = {\n",
        "        'z1': z1, 'h1': h1,\n",
        "        'z2': z2, 'h2': h2,\n",
        "        'z3': z3\n",
        "    }\n",
        "    return out, cache\n",
        "\n",
        "# </DO NOT CHANGE>"
      ],
      "metadata": {
        "id": "KfUeRZZXdvHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After having built the MLP $\\mathbf{\\hat{y}} = f(\\mathbf{x}; \\theta)$, we would like to train it, *i.e.*, find parameters $\\theta$ that minimize an objective function $J(\\theta)$ defined on training data $\\{(\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)})\\}_{i\\leq N}$ as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\theta^* &= \\mathrm{arg}\\min_\\theta J(\\theta),\\\\\n",
        "J(\\theta) &= \\frac{1}{N}\\sum_{i=1}^N{J^{(i)}(\\theta)} = \\frac{1}{N}\\sum_{i=1}^N{\\mathrm{BCELoss}\\left(f(\\mathbf{x}^{(i)}; \\theta), \\mathbf{y}^{(i)}\\right)},\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where the binary cross entropy loss $\\mathrm{BCELoss}\\left(f(\\mathbf{x}^{(i)}; \\theta), \\mathbf{y}^{(i)}\\right)$ measures the mismatch between model prediction $f(\\mathbf{x}^{(i)}; \\theta)\\in[0, 1]$ and binary true label $\\mathbf{y}^{(i)}\\in\\{0, 1\\}$ as follows:\n",
        "\n",
        "$$\n",
        "\\mathrm{BCELoss}\\left(\\mathbf{\\hat{y}}, \\mathbf{y}\\right) = - \\left(\\mathbf{y}\\log\\mathbf{\\hat{y}} + (1-\\mathbf{y})\\log(1 - \\mathbf{\\hat{y}})\\right).\n",
        "$$\n",
        "\n",
        "ðŸ›‘ <mark> TODO 2</mark>: Implement the forward computation of the binary cross entropy loss."
      ],
      "metadata": {
        "id": "KjiBm4DNAC8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass_binary_cross_entropy_loss(out: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Performs binary cross entropy loss computation.\n",
        "    out: model output between 0 and 1 (shape: C)\n",
        "    target: true label between 0 and 1 (shape: C)\n",
        "    Returns:\n",
        "    loss: binary cross entropy loss (shape: C).\n",
        "    \"\"\"\n",
        "    # <TODO>\n",
        "    # Write your implementation here.\n",
        "    # </TODO>\n",
        "    return loss"
      ],
      "metadata": {
        "id": "EOKoMPjz__ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the correctness of your implementation by comparing it to reference results obtained from PyTorch.\n",
        "\n",
        "ðŸ›‘ <mark> TODO 3</mark>: Run the below test code for the forward pass. (No need to code or write anything to solve this todo)"
      ],
      "metadata": {
        "id": "n6gtOPqNFXRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "\n",
        "# Initialize parameters\n",
        "D, H, C = 2, 3, 1\n",
        "reset_seed()\n",
        "W1, b1 = initialize_parameters(D, H)\n",
        "W2, b2 = initialize_parameters(H, H)\n",
        "W3, b3 = initialize_parameters(H, C)\n",
        "\n",
        "# Get a random input and target\n",
        "x = np.random.randn(D)\n",
        "target = np.random.randint(2, size=C)\n",
        "\n",
        "# Compute forward pass\n",
        "out, cache = forward_pass(x, W1, b1, W2, b2, W3, b3)\n",
        "loss = forward_pass_binary_cross_entropy_loss(out, target)\n",
        "\n",
        "# Compare with PyTorch reference output\n",
        "ref_out = np.array([0.49858720162952513])\n",
        "ref_loss = np.array([0.6903255683135151])\n",
        "assert np.allclose(out, ref_out), f'output {out} different from reference output {ref_out} within error tolerance, please check your implementation.'\n",
        "print(f'Test passed, output {out} equal to reference output {ref_out} within error tolerance.')\n",
        "assert np.allclose(loss, ref_loss), f'loss {loss} different from reference loss {ref_loss} within error tolerance, please check your implementation.'\n",
        "print(f'Test passed, loss {loss} equal to reference output {ref_loss} within error tolerance.')\n",
        "\n",
        "# </DO NOT CHANGE>"
      ],
      "metadata": {
        "id": "nl8pnSHB-86b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Backward Computation\n",
        "\n",
        "Up to this point, we have implemented the forward computation of the neural network and the loss function.\n",
        "We will now implement the **backward pass** where the neural network learns by adjusting its parameters.\n",
        "\n",
        "Recall our optimization problem $\\theta^* = \\mathrm{arg}\\min_\\theta J(\\theta)$.\n",
        "Since the objective is non-convex, we would like to optimize it through gradient descent.\n",
        "Specifically, we first initialize $\\theta_0$ randomly, and iteratively run below gradient descent steps with learning rate $\\eta$ until convergence:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} \\leftarrow \\theta_t - \\eta \\frac{\\partial J(\\theta_t)}{\\partial \\theta_t},\n",
        "$$\n",
        "where the gradient is computed as follows:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(\\theta)}{\\partial \\theta} &= \\frac{\\partial}{\\partial \\theta} \\left(\\frac{1}{N}\\sum_{i=1}^N{J^{(i)}(\\theta)} \\right) \\\\\n",
        "&= \\frac{1}{N}\\sum_{i=1}^N{\\frac{\\partial J^{(i)}(\\theta)}{\\partial \\theta}} = \\frac{1}{N}\\sum_{i=1}^N{\\frac{\\partial \\mathrm{BCELoss}\\left(f(\\mathbf{x}^{(i)}; \\theta), \\mathbf{y}^{(i)}\\right)}{\\partial \\theta}}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "In below code, you need to calculate the gradients of the loss for $i$-th training data $J^{(i)}(\\theta) = \\mathrm{BCELoss}\\left(f(\\mathbf{x}^{(i)}; \\theta), \\mathbf{y}^{(i)}\\right)$, written as $\\frac{\\partial J^{(i)}(\\theta)}{\\partial \\theta}$, with respect to each weight and bias parameter, such as $\\frac{\\partial J^{(i)}(\\theta)}{\\partial \\mathbf{W}^{(1)}}$, $\\frac{\\partial J^{(i)}(\\theta)}{\\partial \\mathbf{b}^{(1)}}$, and so on.\n",
        "\n",
        "For this, you need to implement the backpropagation algorithm based on chain rule for partial derivatives.\n",
        "For details of the algorithm, please review the lecture slides *2. Neural Network Basics* and *3. Neural Network Optimization*.\n",
        "\n",
        "For now, don't worry about the averaging over data points $\\frac{\\partial J(\\theta)}{\\partial \\theta} = \\frac{1}{N}\\sum_{i=1}^N{\\frac{\\partial J^{(i)}(\\theta)}{\\partial \\theta}}$.\n",
        "It will be handled in later parts of the code.\n",
        "You only need to calculate $\\frac{\\partial J^{(i)}(\\theta)}{\\partial \\theta}$ given a datapoint $(\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)})$.\n",
        "\n",
        "ðŸ›‘ <mark> TODO 4</mark>: Implement the backward computation of the MLP."
      ],
      "metadata": {
        "id": "GHt0gKZHzbS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass_binary_cross_entropy_loss(out: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Backward pass for binary cross entropy loss.\n",
        "    out: model output between 0 and 1 (shape: C)\n",
        "    target: true label between 0 and 1 (shape: C)\n",
        "    Returns:\n",
        "    dout: gradient w.r.t. out (shape: C).\n",
        "    \"\"\"\n",
        "    # <TODO>\n",
        "    # Write your implementation here.\n",
        "    # </TODO>\n",
        "    return dout\n",
        "\n",
        "\n",
        "def backward_pass_sigmoid(dout, x):\n",
        "    \"\"\"Backward pass for the sigmoid activation.\n",
        "    dout: Upstream gradient (shape: H)\n",
        "    x: input data (shape: H)\n",
        "    Returns:\n",
        "    dx: gradient w.r.t x (shape: H)\n",
        "    \"\"\"\n",
        "    # <TODO>\n",
        "    # Write your implementation here.\n",
        "    # </TODO>\n",
        "    return dx\n",
        "\n",
        "\n",
        "def backward_pass_relu(dout, x):\n",
        "    \"\"\"Backward pass for the ReLU activation.\n",
        "    dout: Upstream gradient (shape: H)\n",
        "    x: input data (shape: H)\n",
        "    Returns:\n",
        "    dx: gradient w.r.t x (shape: H)\n",
        "    \"\"\"\n",
        "    # <TODO>\n",
        "    # Write your implementation here.\n",
        "    # </TODO>\n",
        "    return dx\n",
        "\n",
        "\n",
        "def backward_pass_linear(dout, x, W, b):\n",
        "    \"\"\"Backward pass for the linear layer.\n",
        "    dout: Upstream gradient (shape: H)\n",
        "    x: input data (shape: D)\n",
        "    W: weight matrix (shape: H x D)\n",
        "    b: bias vector (shape: H)\n",
        "    Returns:\n",
        "    dx: gradient w.r.t x (shape: D)\n",
        "    dW: gradient w.r.t W (shape: H x D)\n",
        "    db: gradient w.r.t b (shape: H)\n",
        "    \"\"\"\n",
        "    # <TODO>\n",
        "    # Write your implementation here.\n",
        "    # </TODO>\n",
        "    return dx, dW, db"
      ],
      "metadata": {
        "id": "r80UvyTV7xUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "\n",
        "def backward_pass(dout, x, W1, b1, W2, b2, W3, b3, cache):\n",
        "    \"\"\"Complete backward pass through the 2-hidden layer MLP.\n",
        "    dout: Upstream gradient (usually gradient of loss w.r.t output, shape: 1)\n",
        "    x: input data (shape: D)\n",
        "    W1, b1, W2, b2, W3, b3: weights and biases\n",
        "    cache: a dictionary containing 'z1', 'h1', 'z2', 'h2', 'z3' which are intermediate outputs of forward pass\n",
        "    Returns:\n",
        "    gradients: a dictionary containing gradients for all weights and biases\n",
        "    \"\"\"\n",
        "    # Output layer gradients\n",
        "    dz3 = backward_pass_sigmoid(dout, cache['z3'])\n",
        "    dh2, dW3, db3 = backward_pass_linear(dz3, cache['h2'], W3, b3)\n",
        "\n",
        "    # Second hidden layer gradients\n",
        "    dz2 = backward_pass_relu(dh2, cache['z2'])\n",
        "    dh1, dW2, db2 = backward_pass_linear(dz2, cache['h1'], W2, b2)\n",
        "\n",
        "    # First hidden layer gradients\n",
        "    dz1 = backward_pass_relu(dh1, cache['z1'])\n",
        "    dx, dW1, db1 = backward_pass_linear(dz1, x, W1, b1)\n",
        "\n",
        "    gradients = {\n",
        "        'dW1': dW1, 'db1': db1,\n",
        "        'dW2': dW2, 'db2': db2,\n",
        "        'dW3': dW3, 'db3': db3\n",
        "    }\n",
        "    return gradients\n",
        "\n",
        "# </DO NOT CHANGE>"
      ],
      "metadata": {
        "id": "iS0xAzBzd1Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the correctness of your implementation by comparing it to reference results obtained from automatic differentiation of PyTorch.\n",
        "\n",
        "ðŸ›‘ <mark> TODO 5</mark>: Run the below test code for the backward pass. (No need to code or write anything to solve this todo)"
      ],
      "metadata": {
        "id": "Uc7nr02ddjfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "\n",
        "# Initialize parameters\n",
        "D, H, C = 2, 3, 1\n",
        "reset_seed()\n",
        "W1, b1 = initialize_parameters(D, H)\n",
        "W2, b2 = initialize_parameters(H, H)\n",
        "W3, b3 = initialize_parameters(H, C)\n",
        "\n",
        "# Get a random input and target\n",
        "x = np.random.randn(D)\n",
        "target = np.random.randint(2, size=C)\n",
        "\n",
        "# Compute forward pass\n",
        "out, cache = forward_pass(x, W1, b1, W2, b2, W3, b3)\n",
        "loss = forward_pass_binary_cross_entropy_loss(out, target)\n",
        "\n",
        "# Compute backward pass\n",
        "dout = backward_pass_binary_cross_entropy_loss(out, target)\n",
        "gradients = backward_pass(dout, x, W1, b1, W2, b2, W3, b3, cache)\n",
        "\n",
        "# Compare with reference gradients\n",
        "ref_dout = np.array([1.99436473])\n",
        "ref_dW1 = np.array([[4.4305505e-06, -4.5973820e-05], [0, 0], [0, 0]])\n",
        "ref_db1 = np.array([3.9942725e-05, 0, 0])\n",
        "ref_dW2 = np.array([[0, 0, 0], [0, 0, 0], [-0.0001274, 0, 0]])\n",
        "ref_db2 = np.array([[0, 0, -0.00710361]])\n",
        "ref_dW3 = np.array([[0, 0, 0.00725726]])\n",
        "ref_db3 = np.array([0.4985872])\n",
        "ref_gradients = {\n",
        "    'dW1': ref_dW1, 'db1': ref_db1,\n",
        "    'dW2': ref_dW2, 'db2': ref_db2,\n",
        "    'dW3': ref_dW3, 'db3': ref_db3\n",
        "}\n",
        "assert np.allclose(dout, ref_dout), f'gradient dout:\\n{dout}\\ndifferent from reference gradient\\n{ref_dout}\\nwithin error tolerance, please check your implementation.'\n",
        "print(f'Test passed, gradient dout equal to reference gradient within error tolerance.')\n",
        "for name, grad in gradients.items():\n",
        "    ref_grad = ref_gradients[name]\n",
        "    assert np.allclose(grad, ref_grad), f'gradient {name}:\\n{grad}\\ndifferent from reference gradient\\n{ref_grad}\\nwithin error tolerance, please check your implementation.'\n",
        "    print(f'Test passed, gradient {name} equal to reference gradient within error tolerance.')\n",
        "\n",
        "# </DO NOT CHANGE>"
      ],
      "metadata": {
        "id": "jzlnkB6zMFRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Gradient Descent\n",
        "\n",
        "With the forward and backward passes correctly implemented, we can now implement the gradient descent algorithm to solve our optimization $\\theta^* = \\mathrm{arg}\\min_\\theta J(\\theta)$.\n",
        "We first initialize $\\theta_0$ randomly, and iteratively run below gradient descent steps with learning rate $\\eta$ until convergence:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} \\leftarrow \\theta_t - \\eta \\frac{\\partial J(\\theta_t)}{\\partial \\theta_t}.\n",
        "$$\n",
        "\n",
        "In the below code, you need to implement gradient descent steps that update current parameters $\\theta_t$ given gradients $\\frac{\\partial J(\\theta_t)}{\\partial \\theta_t}$ and learning rate $\\eta$.\n",
        "\n",
        "ðŸ›‘ <mark> TODO 6</mark>: Implement the gradient descent update steps for the MLP."
      ],
      "metadata": {
        "id": "aoz3PuUzeBwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate):\n",
        "    \"\"\"A single step of gradient descent update.\n",
        "    W1, b1, W2, b2, W3, b3: weights and biases\n",
        "    dW1, db1, dW2, db2, dW3, db3: gradient of the object function w.r.t. weights and biases\n",
        "    learning_rate: learning rate\n",
        "    Returns:\n",
        "    W1, b1, W2, b2, W3, b3: updated weights and biases\n",
        "    \"\"\"\n",
        "    # <TODO>\n",
        "    # Write your implementation here.\n",
        "    # <\\TODO>\n",
        "    return W1, b1, W2, b2, W3, b3"
      ],
      "metadata": {
        "id": "xaon3_ZHinm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4. Training and Testing on Synthetic Binary Classification\n",
        "\n",
        "Let's test your implementation by training and testing the MLP on a synthetic binary classification dataset.\n",
        "\n",
        "ðŸ›‘ <mark> TODO 7</mark>: Run the below test code for MLP and gradient descent. (No need to code or write anything to solve this todo)"
      ],
      "metadata": {
        "id": "1Gsi4gP7fc4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "\n",
        "# Main function to train the network\n",
        "def train_network(X_train, Y_train, W1, b1, W2, b2, W3, b3, gradient_steps, learning_rate):\n",
        "\n",
        "    # Run training with gradient descent\n",
        "    for gradient_step in range(gradient_steps):\n",
        "        loss_list = []\n",
        "        gradients_list = []\n",
        "\n",
        "        for x, target in zip(X_train, Y_train):\n",
        "            # Compute forward pass\n",
        "            out, cache = forward_pass(x, W1, b1, W2, b2, W3, b3)\n",
        "            loss = forward_pass_binary_cross_entropy_loss(out, target)\n",
        "\n",
        "            # Compute backward pass\n",
        "            dout = backward_pass_binary_cross_entropy_loss(out, target)\n",
        "            gradients = backward_pass(dout, x, W1, b1, W2, b2, W3, b3, cache)\n",
        "\n",
        "            loss_list.append(loss)\n",
        "            gradients_list.append(gradients)\n",
        "\n",
        "        # Average gradients over dataset\n",
        "        dW1 = np.mean(np.stack([gradients['dW1'] for gradients in gradients_list], axis=0), axis=0)\n",
        "        db1 = np.mean(np.stack([gradients['db1'] for gradients in gradients_list], axis=0), axis=0)\n",
        "        dW2 = np.mean(np.stack([gradients['dW2'] for gradients in gradients_list], axis=0), axis=0)\n",
        "        db2 = np.mean(np.stack([gradients['db2'] for gradients in gradients_list], axis=0), axis=0)\n",
        "        dW3 = np.mean(np.stack([gradients['dW3'] for gradients in gradients_list], axis=0), axis=0)\n",
        "        db3 = np.mean(np.stack([gradients['db3'] for gradients in gradients_list], axis=0), axis=0)\n",
        "\n",
        "        # Gradient descent step\n",
        "        W1, b1, W2, b2, W3, b3 = update_parameters(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate)\n",
        "\n",
        "        # Print loss every 100 gradient descent steps\n",
        "        if gradient_step % 100 == 0:\n",
        "            print(f\"Step {gradient_step}, Loss: {np.mean(loss_list)}\")\n",
        "\n",
        "    # Final loss for correctness test\n",
        "    final_loss = np.mean(loss_list)\n",
        "\n",
        "    return W1, b1, W2, b2, W3, b3, final_loss\n",
        "\n",
        "# <\\DO NOT CHANGE>"
      ],
      "metadata": {
        "id": "43iEq19HlMpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "\n",
        "# Setup train and test datasets\n",
        "X, Y = sklearn.datasets.make_moons(n_samples=500, noise=0.1, random_state=42)\n",
        "Y = np.expand_dims(Y, 1)\n",
        "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=24)\n",
        "\n",
        "# Visualize dataset\n",
        "plt.figure()\n",
        "plt.title('Train data with true labels')\n",
        "scatter = plt.scatter(X[:, 0], X[:, 1], c=Y)\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=['y=0', 'y=1'])\n",
        "plt.show()\n",
        "\n",
        "# Initialize parameters\n",
        "D, H, C = X_train.shape[-1], 32, Y_train.shape[-1]\n",
        "reset_seed()\n",
        "W1, b1 = initialize_parameters_xavier(D, H)\n",
        "W2, b2 = initialize_parameters_xavier(H, H)\n",
        "W3, b3 = initialize_parameters_xavier(H, C)\n",
        "\n",
        "# Run training with gradient descent\n",
        "gradient_steps = 1000\n",
        "learning_rate = 0.1\n",
        "W1, b1, W2, b2, W3, b3, final_loss = train_network(X_train, Y_train, W1, b1, W2, b2, W3, b3, gradient_steps, learning_rate)\n",
        "\n",
        "# Run predictions on the test data\n",
        "out_list = []\n",
        "for x, target in zip(X_test, Y_test):\n",
        "    out, _ = forward_pass(x, W1, b1, W2, b2, W3, b3)\n",
        "    out_list.append(out)\n",
        "predictions = (np.stack(out_list, axis=0) > 0.5).astype(int)\n",
        "accuracy = np.mean(predictions == Y_test)\n",
        "\n",
        "# Visualize predictions\n",
        "plt.figure()\n",
        "plt.title('Model predictions for test data')\n",
        "scatter = plt.scatter(X_test[:, 0], X_test[:, 1], c=predictions)\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=['pred=0', 'pred=1'])\n",
        "plt.show()\n",
        "\n",
        "# Test correctness\n",
        "ref_final_loss = 0.019107356285473685\n",
        "ref_accuracy = 1.\n",
        "assert np.allclose(final_loss, ref_final_loss), f'Final train loss: {final_loss} different from reference loss {ref_final_loss}, please check your implementation.'\n",
        "print(f'Test passed, final train loss: {final_loss}.')\n",
        "assert np.allclose(accuracy, ref_accuracy), f'Test accuracy: {100 * accuracy}% different from reference accuracy {100 * ref_accuracy}, please check your implementation.'\n",
        "print(f'Test passed, test accuracy: {100 * accuracy}%.')\n",
        "\n",
        "# <\\DO NOT CHANGE>"
      ],
      "metadata": {
        "id": "OHgYH36UlO25"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}